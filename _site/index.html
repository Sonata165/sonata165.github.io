<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Longshen Ou (欧龙燊) | Welcome to my portfolio! Here, you’ll discover my journey in creating innovative and harmonious connections between the world of classical instruments and cutting-edge AI research.</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Longshen Ou (欧龙燊)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Welcome to my portfolio! Here, you’ll discover my journey in creating innovative and harmonious connections between the world of classical instruments and cutting-edge AI research." />
<meta property="og:description" content="Welcome to my portfolio! Here, you’ll discover my journey in creating innovative and harmonious connections between the world of classical instruments and cutting-edge AI research." />
<link rel="canonical" href="http://localhost:4000/" />
<meta property="og:url" content="http://localhost:4000/" />
<meta property="og:site_name" content="Longshen Ou (欧龙燊)" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Longshen Ou (欧龙燊)" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","description":"Welcome to my portfolio! Here, you’ll discover my journey in creating innovative and harmonious connections between the world of classical instruments and cutting-edge AI research.","headline":"Longshen Ou (欧龙燊)","name":"Longshen Ou (欧龙燊)","url":"http://localhost:4000/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Longshen Ou (欧龙燊)" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Longshen Ou (欧龙燊)</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/musician">As Musician</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="home"><!-- 
Sections: 
Intro
News
Publication
Other Projects
Honors and Awards
Teaching
Academic Reviewers
 -->

<!-- Google Analytics tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-MK1PD93QHP"></script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-MK1PD93QHP');
</script>

<!-- Navigator -->
<div style="text-align:center;">
[ <a href="#recent-news">News</a> | <a href="#publications">Publications</a> | <a href="#other-projects">Projects</a> | <a href="#honors-and-awards">Awards</a> | <a href="#teaching">Teaching</a> | <a href="#academic-reviewers">Reviewer</a> ]
</div>

<h1 id="im-an-ai-researcher-in-nlp-and-audio-processing">I’m an AI Researcher in NLP and Audio Processing.</h1>

<p><b>[Open to Work] I’m open to Singapore-based part-time roles, starting from July 2025 </b></p>

<!-- Style for image width at intro (responsive) -->
<!-- <style>
    .content {
      display: flex;
      align-items: center;
    }

    .image {
      width: 200px;
      margin-right: 30px;
    }

    @media screen and (max-width: 600px) {
    .content {
      display: block;
      text-align: left;
    }

    .image {
      display: block;
      width: 50%;
      height: auto;
      margin: 0 auto 30px;
    }
  }
</style> -->

<!-- Intro -->
<div class="content" style="display: flex; align-items: center;">
  <div style="width: 33%; margin-right: 30px;">
    <img class="image" src="/assets/images/longshen2022.jpg" alt="Image description" />
  </div>
  <div style="width: 66%;">
    <p>
    I am currently a proud PhD candidate (last year) in the <a href="https://smcnus.comp.nus.edu.sg/" target="_blank">Sound and Music Computing Lab</a>, School of Computing, National University of Singapore, advised by Prof. <a href="https://www.comp.nus.edu.sg/cs/people/wangye/" target="_blank">Ye Wang (王晔)</a>. My research focuses on advancing music information retrieval (MIR), audio and speech processing, and natural language processing (NLP) through deep learning techniques. I am particularly interested in the automatic transcription, generation, and translation of music and lyrics, with a strong emphasis on self-supervised learning (SSL), transfer learning, multimodal learning, and integrating controllability into SSL models. Prior to joining NUS, I earned my Bachelor's degree in Computer Science with honors from the Harbin Institute of Technology and completed my bachelor's thesis on piano music transcription at the Auditory Intelligence Research Center, advised by Prof. <a href="http://homepage.hit.edu.cn/hanjiqing?lang=zh" target="_blank">Jiqing Han (韩纪庆)</a>.
    </p>
  </div>
</div>

<div>
  <p>
  Moreover, I am a professional-level violinist and guitarist. Please visit the "<a href="/musician">As Musician</a>" page to explore my music portfolio. I feel incredibly fortunate to have discovered my passion for music and to have the opportunity to work and conduct research in a music-related field. My love for music energizes and motivates me to continually grow and excel in this area.
  </p>
</div>

<!-- Contact -->
<!-- Email ` Scholar ` CV/LinkedIn ` Twitter ` Wechat ` Bilibili ` Youtube -->
<div style="text-align:center;">
  <a href="mailto:oulongshen@u.nus.edu">Email</a> &middot; 
  <a href="https://scholar.google.com/citations?user=hf-xY6gAAAAJ" target="_blank">Google Scholar</a> &middot; 
  <a href="https://www.linkedin.com/in/longshen-ou/" target="_blank">LinkedIn</a> &middot; 
  <a href="assets/pdf/CV/2025-01 Longshen.pdf" target="_blank">Full CV</a> &middot; 
  <a href="https://twitter.com/LongshenO" target="_blank">Twitter</a> &middot; 
  <a href="/assets/images/WechatQR.jpeg" target="_blank">WeChat</a>
</div>

<p><br /></p>

<h1 id="recent-news">Recent News</h1>

<div style="height: 100px; overflow-y: scroll; border: 1px solid #ccc; padding: 10px; font-family: Times New Roman;background-color: gainsboro;">
<ul>
    <li> <b>[2024.12]</b> The paper <i>Lead Instrument Detection from Multitrack Music</i> is accepted by ICASSP 2025. </li>
    <li> <b>[2024.12]</b> <a href="https://github.com/Sonata165/REMI-z" target="_blank">REMI-z tokenizer and MultiTrack music data structure</a> is now available in <a href="https://pypi.org/project/REMI-z/" target="_blank">PyPI</a>. This is my first open source project on pip :) </li>
    <li> <b>[2024.8]</b> I started my internship at Sony Computer Science Laboratories. </li>
    <li> <b>[2024.5]</b> I started my internship at YAMAHA at Hamamatsu, Shizuoka, Japan. </li>
    <li> <b>[2024.3]</b> The paper <i>DNA Storage Toolkit: A Modular End-to-End DNA Data Storage Codec and Simulator</i> is accepted by ISPASS 2024. Congratulations to <a href="https://prongs1996.github.io/" target="_blank">Puru Sharma</a>! </li>
    <li> <b>[2024.3]</b> The paper <i>Automatic Lyric Transcription and Automatic Music Transcription from Multimodal Singing</i> is accepted by ACM TOMM. Congratulations to my colleague Xiangming! </li>
    <li> <b>[2023.10]</b> My short paper <i>Singable and Controllable Neural Lyric Translation: a Late-Breaking Showcase</i> is accepted by ISMIR 2023 Late Breaking Demo. </li>
    <li> <b>[2023.6]</b> One full paper was rejected by ISMIR 2023. Sadge! </li>
    <li> <b>[2023.5]</b> I passed the Qualification Exam. Now I am a PhD candidate! </li>
    <li> <b>[2023.5]</b> My paper <i>Songs Across Borders: Singable and Controllable Neural Lyric Translation</i> is accepted by ACL 2023. </li>
    <li> <b>[2023.1]</b> I receive Research Achievement Award (2022/2023) from School of Computing, NUS. </li>
    <li> <b>[2022.12]</b> I'm attending ISMIR 2023 at Bengaluru, India. </li>
    <li> <b>[2022.11]</b> Our ACM Multimedia paper receives the top paper award (2% of accepted full papers). </li>
    <li> <b>[2022.10]</b> I'm attending ACM Multimedia at Lisbon, Portugal. </li>
    <li> <b>[2022.7]</b> An extension work of our previous paper, <i>Transfer Learning of wav2vec 2.0 for Automatic Lyric Transcription</i> is acctepted by ISMIR 2023.</li>
    <li> <b>[2022.7]</b> My paper collaborated with <a href="https://guxm2021.github.io/" target="_blank">Xiangming Gu</a>, <i>MM-ALT: A multimodal automatic lyric transcription system</i> is accepted by ACM Multimedia 2022. </li>
    <li> <b>[2022.5]</b> I'm attending ICASSP 2022 at Singapore. </li>
    <li> <b>[2022.1]</b> My first paper, which achieves another SOTA on piano music transcription, is accepted by ICASSP 2022.</li>
    <li> <b>[2022.1]</b> I start my PhD journey in NUS SMCL, advised by <a href="https://www.comp.nus.edu.sg/cs/people/wangye/" target="_blank">Prof. Wang Ye</a>. </li>
    <li> <b>[2021.8]</b> I join National University of Singapore as a student in Master of Computing program (AI track), start my research in <a href="https://smcnus.comp.nus.edu.sg/" target="_blank"> Sound and Music Computing Lab </a>.</li>
</ul>
</div>

<p><br /></p>

<h1 id="publications">Publications</h1>
<ul>
  <li>
    <p><strong><a href="assets/pdf/papers/Lead_Instrument_Detection_from_Multitrack_Music.pdf" target="_blank">Lead Instrument Detection from Multitrack Music</a></strong><br />
<strong>Longshen Ou</strong>, <a href="https://yuu-t.github.io/" target="_blank">Yu Takahashi</a>, and <a href="https://www.comp.nus.edu.sg/cs/people/wangye/" target="_blank">Ye Wang</a><br />
<em>2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2025)</em><br />
[<a href="assets/pdf/papers/Lead_Instrument_Detection_from_Multitrack_Music.pdf" target="_blank">preprint</a> | code &amp; dataset]
<!-- 2024.08.28 --></p>
  </li>
  <li>
    <p><strong><a href="https://ieeexplore.ieee.org/abstract/document/10590050/" target="_blank">DNA Storage Toolkit: A Modular End-to-End DNA Data Storage Codec and Simulator</a></strong><br />
<a href="https://prongs1996.github.io/" target="_blank">Puru Sharma</a>, Gary Goh, Bin Gao, <strong>Longshen Ou</strong>, Dehui Lin, Deepak Sharma, Djordje Jevdjic<br />
<em>2024 IEEE Int’l Symposium on Performance Analysis of Systems and Software (ISPASS 2024)</em><br />
[<a href="https://prongs1996.github.io/assets/pdf/ispass24presentation.pdf" target="_blank">slides</a>]</p>
  </li>
  <li>
    <p><strong><a href="https://dl.acm.org/doi/abs/10.1145/3651310" target="_blank">Automatic Lyric Transcription and Automatic Music Transcription from Multimodal Singing</a></strong><br />
<a href="https://guxm2021.github.io/" target="_blank">Xiangming Gu</a>, <strong>Longshen Ou</strong>, <a href="https://scholar.google.com/citations?user=Ffq7lrcAAAAJ" target="_blank">Wei Zeng</a>, Jianan Zhang, <a href="https://nic-wong.carrd.co/" target="_blank">Nicholas Wong</a>, <a href="https://www.comp.nus.edu.sg/cs/people/wangye/" target="_blank">Ye Wang</a><br />
<em>ACM Transactions on Multimedia Computing, Communications and Applications (TOMM 2024)</em><br />
[<a href="https://github.com/guxm2021/SVT_SpeechBrain" target="_blank">code</a> | <a href="https://arxiv.org/abs/2304.12082" target="_blank">ArXiv</a>]</p>
  </li>
  <li>
    <p><strong><a href="https://aclanthology.org/2023.acl-long.27/" target="_blank">Songs Across Borders: Singable and Controllable Neural Lyric Translation</a></strong><br />
<strong>Longshen Ou</strong>, <a href="https://dblp.org/pid/179/9890.html" target="_blank">Xichu Ma</a>, <a href="https://www.comp.nus.edu.sg/~kanmy/" target="_blank">Min-Yen Kan</a>, <a href="https://www.comp.nus.edu.sg/cs/people/wangye/" target="_blank">Ye Wang</a><br />
<em>The 61st Annual Meeting of the Association for Computational Linguistics (ACL 2023)</em><br />
[<a href="/lyric_translation">demo</a> | <a href="https://github.com/Sonata165/ControllableLyricTranslation">code</a>]</p>
  </li>
  <li>
    <p><strong><a href="https://arxiv.org/abs/2207.09747">Transfer Learning of wav2vec 2.0 for Automatic Lyric Transcription</a></strong><br />
<strong>Longshen Ou</strong>*, <a href="https://guxm2021.github.io/" target="_blank">Xiangming Gu</a>*, and <a href="https://www.comp.nus.edu.sg/cs/people/wangye/" target="_blank">Ye Wang</a><br />
<em>Proceedings of the 23rd International Society for Music Information Retrieval Conf. (ISMIR 2022)</em><br />
[<a href="https://github.com/guxm2021/ALT_SpeechBrain">code</a>]</p>
  </li>
  <li>
    <p><strong><a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548411">MM-ALT: A Multimodal Automatic Lyric Transcription System</a></strong> (<em><span style="color:red">Oral, Top Paper Award</span></em>)<br />
<a href="https://guxm2021.github.io/" target="_blank">Xiangming Gu</a>*, <strong>Longshen Ou</strong>*, <a href="https://www.linkedin.com/in/danielle-ong-854b88177/" target="_blank">Danielle Ong</a>, and <a href="https://www.comp.nus.edu.sg/cs/people/wangye/" target="_blank">Ye Wang</a><br />
<em>Proceedings of the 30th ACM International Conference on Multimedia (ACM Multimedia 2022)</em> <br />
[<a href="https://n20em.github.io/" target="_blank">demo</a> | <a href="https://github.com/guxm2021/MM_ALT" target="_blank">code</a> | <a href="https://zenodo.org/record/7545968" target="_blank">dataset</a> | <a href="https://www.comp.nus.edu.sg/news/features/2023-marvellous-richness-wye/" target="_blank">press</a>]</p>
  </li>
  <li>
    <p><a href="https://ieeexplore.ieee.org/abstract/document/9746789"><strong>Exploring Transformer’s Potential on Automatic Piano Transcription</strong></a><br />
<strong>Longshen Ou</strong>, <a href="https://www.linkedin.com/in/zi-yi-guo/" target="_blank">Ziyi Guo</a>, <a href="https://www.eecs.qmul.ac.uk/~emmanouilb/" target="_blank">Emmanouil Benetos</a>, <a href="https://dblp.org/pid/h/JiqingHan.html" target="_blank">Jiqing Han</a>, and <a href="https://www.comp.nus.edu.sg/cs/people/wangye/" target="_blank">Ye Wang</a>  <br />
<em>2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2022)</em></p>
  </li>
</ul>

<h2 id="preprint">Preprint</h2>

<ul>
  <li>
    <p><strong><a href="assets/pdf/papers/Automatic_Arrangement.pdf" target="_blank">Unifying Multitrack Music Arrangement via Reconstruction Fine-Tuning and Efficient Tokenization</a></strong><br />
<strong>Longshen Ou</strong>, <a href="https://zhaojw1998.github.io/" target="_blank">Jingwei Zhao</a>, <a href="https://scholar.google.com/citations?user=QFOh6EgAAAAJ&amp;hl=en" target="_blank">Ziyu Wang</a>, <a href="https://mbzuai.ac.ae/study/faculty/dr-gus-xia/" target="_blank">Gus Xia</a>, and <a href="https://www.comp.nus.edu.sg/cs/people/wangye/" target="_blank">Ye Wang</a><br />
<em>arXiv:2408.15176 (2024)</em></p>
  </li>
  <li>
    <p><strong><a href="https://arxiv.org/abs/2307.02146" target="_blank">LOAF-M2L: Joint Learning of Wording and Formatting for Singable Melody-to-Lyric Generation</a></strong><br />
<strong>Longshen Ou</strong>, <a href="https://dblp.org/pid/179/9890.html" target="_blank">Xichu Ma</a>, and <a href="https://www.comp.nus.edu.sg/cs/people/wangye/" target="_blank">Ye Wang</a><br />
<em>arXiv:2307.02146 (2023)</em></p>
  </li>
</ul>

<!-- - **[Automatic Hyper-Parameter Optimization Based on Mapping Discovery from Data to Hyper-Parameters](https://arxiv.org/abs/2003.01751){:target="_blank"}**  
  [Bozhou Chen](https://www.researchgate.net/profile/Bozhou-Chen){:target="_blank"}, [Kaixin Zhang](https://www.researchgate.net/profile/Kaixin-Zhang-6){:target="_blank"}, **Longshen Ou**, [Chenmin Ba](https://dblp.uni-trier.de/pid/259/9983.html){:target="_blank"}, [Hongzhi Wang](https://dblp.org/pid/81/940.html){:target="_blank"}, and [Chunnan Wang](https://scholar.google.com/citations?user=F0xRt20AAAAJ&hl=en){:target="_blank"}.  
  *arXiv:2003.01751* (2020) -->

<h2 id="demo--workshop-papers">Demo &amp; Workshop Papers</h2>
<ul>
  <li><strong><a href="https://ismir2023program.ismir.net/lbd_347.html" target="_blank">Singable and Controllable Neural Lyric Translation: a Late-Breaking Showcase</a></strong><br />
<strong>Longshen Ou</strong>, <a href="https://dblp.org/pid/179/9890.html" target="_blank">Xichu Ma</a>, and <a href="https://www.comp.nus.edu.sg/cs/people/wangye/" target="_blank">Ye Wang</a><br />
<em>Late Breaking Demo at the 24rd Int. Society for Music Information Retrieval Conf. (ISMIR 2023)</em></li>
</ul>

<p><br /></p>

<h1 id="other-projects">Other Projects</h1>
<ul>
  <li>
    <p><a href="https://github.com/Sonata165/REMI-z" target="_blank"><strong><em>REMI-z</em> Tokenizer and <em>MultiTrack</em> music data structure</strong></a><br />
This tool helps to convert your music between MIDI and REMI-z representation, which is an efficient sequence representation of multitrack music, meanwhile facilitate manipulate the music at bar level.<br />
[<a href="https://github.com/Sonata165/REMI-z">github</a> | <a href="https://pypi.org/project/REMI-z/">PyPI</a>]</p>
  </li>
  <li>
    <p><a href="https://github.com/Sonata165/GuitarFret" target="_blank"><strong>GuitarFret</strong></a><br />
With this guitar fretboard simulator on your laptop, never worry about composing without a guitar around you!</p>
  </li>
  <li>
    <p><a href="https://github.com/Sonata165/DNA-Storage-Simulation" target="_blank"><strong>DNA Storage Simulation</strong></a><br />
  DNA-based storage systems present unique challenges, as reading and writing operations can sometimes result in alterations to the original information. To model the changes introduced by such storage systems in a wet lab environment, we designed a simulation system to emulate DNA behavioral changes. This system includes a rule-based method, a Multi-Layer Perceptron (MLP) method, and a sequence-to-sequence attention-based Recurrent Neural Network (RNN). The experiments based on the Microsoft Nanopore dataset shows the sequence-to-sequence method is highly effective.</p>
  </li>
  <li>
    <p><a href="https://github.com/Sonata165/MusicRecommenderGCN" target="_blank"><strong>GNN-based Music Recommender</strong></a><br />
  This project aims to tackle the music artist recommendation challenge using Graph Convolutional Networks (GCNs). By modeling artist and user identities through their interactive relationships, the network predicts affinity scores between users and previously unexplored artists to generate personalized recommendations. I implemented the original GCN as a baseline and proposed three enhancements: incorporating edge weight for aggregation, augmenting edge weight with attention mechanisms, and implementing data augmentation by introducing noise to edge values.</p>
  </li>
</ul>

<p><br /></p>

<h1 id="honors-and-awards">Honors and Awards</h1>
<ul>
  <li><strong>SoC Research Incentive Award</strong>, issued by School of Computing, NUS, 2023.10.</li>
  <li><strong>Research Achievement Award</strong> (2022/2023), issued by School of Computing, NUS, 2023.5.</li>
  <li><strong>Top Paper Award</strong> (2% of accepted full papers), issued by ACM Multimedia 2022, 2022.11.</li>
  <li><strong>Honor Degree of Bachelor of Engineering</strong>, issued by Harbin Institute of Technology Honors School, 2021.6.</li>
  <li><strong>People Scholarship</strong> (6%), issued by Harbin Institute of Technology, 2020.6.</li>
  <li><strong>Third Prize</strong>, Sogou Innovative Practice Project for College Student, 2018.10.</li>
</ul>

<p><br /></p>

<h1 id="teaching">Teaching</h1>
<ul>
  <li>Teaching Assistant, CS4347/5647 Sound and Music Computing (2022/2023 sem 1, 2023/2024 sem 1).</li>
  <li>Teaching Assistant, CS4248 Natural Language Processing (2022/2023 sem 2).</li>
</ul>

<p><br /></p>

<h1 id="academic-reviewers">Academic Reviewers</h1>
<ul>
  <li>IEEE TAFFC (2024)</li>
  <li>EAI ArtsIT 2024</li>
  <li>ACM TOMM (2024)</li>
  <li>ACM Multimedia 2023, 2024</li>
  <li>ACL Rolling Review (2024)</li>
  <li>TASLP (2024)</li>
  <li>ISMIR 2022, 2023</li>
</ul>

<p><br /></p>
</div>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Longshen Ou (欧龙燊)</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Longshen Ou (欧龙燊)</li><li><a class="u-email" href="mailto:oulongshen@u.nus.edu">oulongshen@u.nus.edu</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Welcome to my portfolio! Here, you&#39;ll discover my journey in creating innovative and harmonious connections between the world of classical instruments and cutting-edge AI research. </p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
